data: "ptb" # ptb, wikitext2
model: "LSTM" # LSTM, RANN_TANH, RNN_RELU, LSTM, GRU, Transformer
embedding: "none" # pruning / block / svd / none(default).
embeddings:
  - "encoder"
  - "decoder"
nhead: 2
nlayers: 2
emsize: 650
nhid: 650
lr: 30
lr_decay: 1.15
clip: 0.25
epochs: 100
batch_size: 20
bptt: 35
dropout: 0.5
tied: False
seed: 1111 # none
mode: "train" # train / test / finetune
weights: "none" # pretrained weights
save_path: "baseline.p7" # path to save weights
log_interval: 200
device: "cuda:3"
target_ratio: 20.0
use_eval_gates: False
use_embedding_for_decoder: False
diff_embedding:
  sparsity: 0.99
  reg_weight: 20.0
  gate_clamping:
    - 0.001
    - 1.0
  gate_training_only: False
  #gate_lr: 0.1
  #precomputed_gates: "../../tasks/lm/tf_score_ptb.pkl" 
svd_options:
  rank: 50 # Only for SVDEmbedding
block_options:
  score: "diff_pruning_rank_assignment.pkl"
  refinement: False
  nblocks: 10
  use_clusters: True
word2ket_options:
  rank: 1
  order: 1
smallfry_options:
  nbit: 4
